{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1883dcb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VGGish' from 'keras.applications' (D:\\Program Files\\New folder\\lib\\site-packages\\keras\\applications\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGGish\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'VGGish' from 'keras.applications' (D:\\Program Files\\New folder\\lib\\site-packages\\keras\\applications\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n",
    "from keras.applications import VGGish\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('edited_file.csv')\n",
    "\n",
    "# Assuming the features are in columns 0 to 20, and the labels are in columns 21, 22, and 23\n",
    "X = df.iloc[:, :21].values\n",
    "y_gender = df['emotion'].values  # Assuming 'gender' is in column 21\n",
    "y_age = df['age'].values  # Assuming 'age' is in column 22\n",
    "y_emotion = df['gender'].values  # Assuming 'emotion' is in column 23\n",
    "\n",
    "# Convert categorical labels to one-hot encoding\n",
    "y_gender = to_categorical(y_gender, num_classes=2)\n",
    "y_age = to_categorical(y_age, num_classes=8)\n",
    "y_emotion = to_categorical(y_emotion, num_classes=6)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_gender_train, y_gender_test, y_age_train, y_age_test, y_emotion_train, y_emotion_test = train_test_split(\n",
    "    X, y_gender, y_age, y_emotion, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# VGGish model\n",
    "input_shape = (21, 1)  # Assuming 21 input features\n",
    "base_model = VGGish(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "flat_vgg = Flatten()(base_model.output)\n",
    "\n",
    "# CNN Layers\n",
    "inputs_cnn = Input(shape=(21, 1))\n",
    "conv1 = Conv1D(32, 3, activation='relu')(inputs_cnn)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(64, 3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "flat_cnn = Flatten()(pool2)\n",
    "\n",
    "# Concatenate the outputs of both models\n",
    "merged = Concatenate()([flat_cnn, flat_vgg])\n",
    "\n",
    "# Dense Layers\n",
    "dense1 = Dense(64, activation='relu')(merged)\n",
    "dense2 = Dense(64, activation='relu')(merged)\n",
    "dense3 = Dense(64, activation='relu')(merged)\n",
    "\n",
    "# Output Layers\n",
    "output_gender = Dense(2, activation='softmax', name='gender')(dense1)\n",
    "output_age = Dense(8, activation='softmax', name='age')(dense2)\n",
    "output_emotion = Dense(6, activation='softmax', name='emotion')(dense3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[inputs_cnn, base_model.input], outputs=[output_gender, output_age, output_emotion])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss={'gender': 'categorical_crossentropy', 'age': 'categorical_crossentropy', 'emotion': 'categorical_crossentropy'},\n",
    "              metrics={'gender': 'accuracy', 'age': 'accuracy', 'emotion': 'accuracy'})\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, X_train], [y_gender_train, y_age_train, y_emotion_train], epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480ff5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/audioset/vggish_model/vggish_model.h5\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://storage.googleapis.com/audioset/vggish_model/vggish_model.h5: 404 -- Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:347\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:85\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd:\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    562\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flatten\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Download and convert VGGish model to Keras format\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m vggish_model_path \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvggish_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://storage.googleapis.com/audioset/vggish_model/vggish_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load the VGGish model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m base_model \u001b[38;5;241m=\u001b[39m load_model(vggish_model_path)\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\utils\\data_utils.py:349\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    347\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39merrno, e\u001b[38;5;241m.\u001b[39mreason))\n",
      "\u001b[1;31mException\u001b[0m: URL fetch failure on https://storage.googleapis.com/audioset/vggish_model/vggish_model.h5: 404 -- Not Found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Download and convert VGGish model to Keras format\n",
    "vggish_model_path = tf.keras.utils.get_file(\n",
    "    'vggish_model.h5',\n",
    "    'https://storage.googleapis.com/audioset/vggish_model/vggish_model.h5',\n",
    "    cache_dir='./'\n",
    ")\n",
    "\n",
    "# Load the VGGish model\n",
    "base_model = load_model(vggish_model_path)\n",
    "\n",
    "# Remove the classification layer\n",
    "base_model = tf.keras.Sequential(base_model.layers[:-1])\n",
    "\n",
    "# Add a Flatten layer to the output of VGGish\n",
    "flat_vgg = Flatten()(base_model.output)\n",
    "\n",
    "# Continue with the rest of your model\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9a4e93",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m vggish_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./vggish_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Download the VGGish model using wget\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mwget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvggish_model_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvggish_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load the VGGish model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m base_model \u001b[38;5;241m=\u001b[39m load_model(vggish_model_path)\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\wget.py:526\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, out, bar)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     binurl \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m--> 526\u001b[0m (tmpfile, headers) \u001b[38;5;241m=\u001b[39m \u001b[43mulib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m filename \u001b[38;5;241m=\u001b[39m detect_filename(url, out, headers)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outdir:\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# URL to download the VGGish model\n",
    "vggish_model_url = 'https://github.com/DTaoo/VGGish_Keras/releases/download/v1.0/vggish_model.h5'\n",
    "\n",
    "# Specify the local path where you want to save the downloaded model file\n",
    "vggish_model_path = './vggish_model.h5'\n",
    "\n",
    "# Download the VGGish model using wget\n",
    "wget.download(vggish_model_url, vggish_model_path)\n",
    "\n",
    "# Load the VGGish model\n",
    "base_model = load_model(vggish_model_path)\n",
    "\n",
    "# Remove the classification layer\n",
    "base_model = tf.keras.Sequential(base_model.layers[:-1])\n",
    "\n",
    "# Add a Flatten layer to the output of VGGish\n",
    "flat_vgg = Flatten()(base_model.output)\n",
    "\n",
    "# Continue with the rest of your model\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b445af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=0868b9d0ceb086934d86600c18acb8db8dc602caa03750cb924192923c7ab38e\n",
      "  Stored in directory: c:\\users\\aaryaman\\appdata\\local\\pip\\cache\\wheels\\46\\78\\0e\\8e5e2b500f83a682c8d7e7ce820638cf99faa894a662f71cf0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fba4227",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m y_gender \u001b[38;5;241m=\u001b[39m to_categorical(y_gender, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     20\u001b[0m y_age \u001b[38;5;241m=\u001b[39m to_categorical(y_age, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m y_emotion \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_emotion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m     24\u001b[0m X_train, X_test, y_gender_train, y_gender_test, y_age_train, y_age_test, y_emotion_train, y_emotion_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     25\u001b[0m     X, y_gender, y_age, y_emotion, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\utils\\np_utils.py:74\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m---> 74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[0;32m     76\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('edited_file.csv')\n",
    "\n",
    "# Assuming the features are in columns 0 to 20, and the labels are in columns 21, 22, and 23\n",
    "X = df.iloc[:, :21].values\n",
    "y_gender = df['gender'].values  # Assuming 'gender' is in column 21\n",
    "y_age = df['age'].values  # Assuming 'age' is in column 22\n",
    "y_emotion = df['emotion'].values  # Assuming 'emotion' is in column 23\n",
    "\n",
    "# Convert categorical labels to one-hot encoding\n",
    "y_gender = to_categorical(y_gender, num_classes=2)\n",
    "y_age = to_categorical(y_age, num_classes=8)\n",
    "y_emotion = to_categorical(y_emotion, num_classes=6)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_gender_train, y_gender_test, y_age_train, y_age_test, y_emotion_train, y_emotion_test = train_test_split(\n",
    "    X, y_gender, y_age, y_emotion, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# CNN Layers\n",
    "inputs_cnn = Input(shape=(21, 1))\n",
    "conv1 = Conv1D(32, 3, activation='relu')(inputs_cnn)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(64, 3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "flat_cnn = Flatten()(pool2)\n",
    "\n",
    "# Dense Layers\n",
    "dense1 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense2 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense3 = Dense(64, activation='relu')(flat_cnn)\n",
    "\n",
    "# Output Layers\n",
    "output_gender = Dense(2, activation='softmax', name='gender')(dense1)\n",
    "output_age = Dense(8, activation='softmax', name='age')(dense2)\n",
    "output_emotion = Dense(6, activation='softmax', name='emotion')(dense3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs_cnn, outputs=[output_gender, output_age, output_emotion])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss={'gender': 'categorical_crossentropy', 'age': 'categorical_crossentropy', 'emotion': 'categorical_crossentropy'},\n",
    "              metrics={'gender': 'accuracy', 'age': 'accuracy', 'emotion': 'accuracy'})\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, [y_gender_train, y_age_train, y_emotion_train], epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6406d762",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m y_gender \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m23\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Assuming 'gender' is in column 23\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert categorical labels to one-hot encoding\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m y_emotion \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_emotion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m y_age \u001b[38;5;241m=\u001b[39m to_categorical(y_age, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     21\u001b[0m y_gender \u001b[38;5;241m=\u001b[39m to_categorical(y_gender, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\utils\\np_utils.py:74\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m---> 74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[0;32m     76\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('edited_file.csv')\n",
    "\n",
    "# Assuming the features are in columns 0 to 20, and the labels are in columns 21, 22, and 23\n",
    "X = df.iloc[:, :21].values\n",
    "y_emotion = df.iloc[:, 21].values  # Assuming 'emotion' is in column 21\n",
    "y_age = df.iloc[:, 22].values  # Assuming 'age' is in column 22\n",
    "y_gender = df.iloc[:, 23].values  # Assuming 'gender' is in column 23\n",
    "\n",
    "# Convert categorical labels to one-hot encoding\n",
    "y_emotion = to_categorical(y_emotion, num_classes=6)\n",
    "y_age = to_categorical(y_age, num_classes=8)\n",
    "y_gender = to_categorical(y_gender, num_classes=2)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_emotion_train, y_emotion_test, y_age_train, y_age_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    X, y_emotion, y_age, y_gender, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# CNN Layers\n",
    "inputs_cnn = Input(shape=(21, 1))\n",
    "conv1 = Conv1D(32, 3, activation='relu')(inputs_cnn)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(64, 3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "flat_cnn = Flatten()(pool2)\n",
    "\n",
    "# Dense Layers\n",
    "dense1 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense2 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense3 = Dense(64, activation='relu')(flat_cnn)\n",
    "\n",
    "# Output Layers\n",
    "output_emotion = Dense(6, activation='softmax', name='emotion')(dense1)\n",
    "output_age = Dense(8, activation='softmax', name='age')(dense2)\n",
    "output_gender = Dense(2, activation='softmax', name='gender')(dense3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs_cnn, outputs=[output_emotion, output_age, output_gender])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss={'emotion': 'categorical_crossentropy', 'age': 'categorical_crossentropy', 'gender': 'categorical_crossentropy'},\n",
    "              metrics={'emotion': 'accuracy', 'age': 'accuracy', 'gender': 'accuracy'})\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, [y_emotion_train, y_age_train, y_gender_train], epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5616375d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert categorical labels to one-hot encoding\u001b[39;00m\n\u001b[0;32m     19\u001b[0m num_emotion_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m---> 20\u001b[0m y_emotion \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_emotion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_emotion_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m y_age \u001b[38;5;241m=\u001b[39m to_categorical(y_age, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     22\u001b[0m y_gender \u001b[38;5;241m=\u001b[39m to_categorical(y_gender, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\utils\\np_utils.py:74\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m---> 74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[0;32m     76\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('edited_file.csv')\n",
    "\n",
    "# Assuming the features are in columns 0 to 20, and the labels are in columns 21, 22, and 23\n",
    "X = df.iloc[:, :21].values\n",
    "y_emotion = df.iloc[:, 21].values  # Assuming 'emotion' is in column 21\n",
    "y_age = df.iloc[:, 22].values  # Assuming 'age' is in column 22\n",
    "y_gender = df.iloc[:, 23].values  # Assuming 'gender' is in column 23\n",
    "\n",
    "# Convert categorical labels to one-hot encoding\n",
    "num_emotion_classes = len(df['emotion'].unique())\n",
    "y_emotion = to_categorical(y_emotion, num_classes=num_emotion_classes)\n",
    "y_age = to_categorical(y_age, num_classes=8)\n",
    "y_gender = to_categorical(y_gender, num_classes=2)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_emotion_train, y_emotion_test, y_age_train, y_age_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    X, y_emotion, y_age, y_gender, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# CNN Layers\n",
    "inputs_cnn = Input(shape=(21, 1))\n",
    "conv1 = Conv1D(32, 3, activation='relu')(inputs_cnn)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(64, 3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "flat_cnn = Flatten()(pool2)\n",
    "\n",
    "# Dense Layers\n",
    "dense1 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense2 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense3 = Dense(64, activation='relu')(flat_cnn)\n",
    "\n",
    "# Output Layers\n",
    "output_emotion = Dense(num_emotion_classes, activation='softmax', name='emotion')(dense1)\n",
    "output_age = Dense(8, activation='softmax', name='age')(dense2)\n",
    "output_gender = Dense(2, activation='softmax', name='gender')(dense3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs_cnn, outputs=[output_emotion, output_age, output_gender])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss={'emotion': 'categorical_crossentropy', 'age': 'categorical_crossentropy', 'gender': 'categorical_crossentropy'},\n",
    "              metrics={'emotion': 'accuracy', 'age': 'accuracy', 'gender': 'accuracy'})\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, [y_emotion_train, y_age_train, y_gender_train], epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a883206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 4 6 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(df['emotion'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78926126",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert categorical labels to one-hot encoding\u001b[39;00m\n\u001b[0;32m     19\u001b[0m num_emotion_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m---> 20\u001b[0m y_emotion \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_emotion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_emotion_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m y_age \u001b[38;5;241m=\u001b[39m to_categorical(y_age, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     22\u001b[0m y_gender \u001b[38;5;241m=\u001b[39m to_categorical(y_gender, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\utils\\np_utils.py:74\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m---> 74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[0;32m     76\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('edited_file.csv')\n",
    "\n",
    "# Assuming the features are in columns 0 to 20, and the labels are in columns 21, 22, and 23\n",
    "X = df.iloc[:, :21].values\n",
    "y_emotion = df.iloc[:, 21].values  # Assuming 'emotion' is in column 21\n",
    "y_age = df.iloc[:, 22].values  # Assuming 'age' is in column 22\n",
    "y_gender = df.iloc[:, 23].values  # Assuming 'gender' is in column 23\n",
    "\n",
    "# Convert categorical labels to one-hot encoding\n",
    "num_emotion_classes = len(df['emotion'].unique())\n",
    "y_emotion = to_categorical(y_emotion, num_classes=num_emotion_classes)\n",
    "y_age = to_categorical(y_age, num_classes=8)\n",
    "y_gender = to_categorical(y_gender, num_classes=2)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_emotion_train, y_emotion_test, y_age_train, y_age_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    X, y_emotion, y_age, y_gender, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# CNN Layers\n",
    "inputs_cnn = Input(shape=(21, 1))\n",
    "conv1 = Conv1D(32, 3, activation='relu')(inputs_cnn)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(64, 3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "flat_cnn = Flatten()(pool2)\n",
    "\n",
    "# Dense Layers\n",
    "dense1 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense2 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense3 = Dense(64, activation='relu')(flat_cnn)\n",
    "\n",
    "# Output Layers\n",
    "output_emotion = Dense(num_emotion_classes, activation='softmax', name='emotion')(dense1)\n",
    "output_age = Dense(8, activation='softmax', name='age')(dense2)\n",
    "output_gender = Dense(2, activation='softmax', name='gender')(dense3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs_cnn, outputs=[output_emotion, output_age, output_gender])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss={'emotion': 'categorical_crossentropy', 'age': 'categorical_crossentropy', 'gender': 'categorical_crossentropy'},\n",
    "              metrics={'emotion': 'accuracy', 'age': 'accuracy', 'gender': 'accuracy'})\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, [y_emotion_train, y_age_train, y_gender_train], epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bbffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 21, 1)]              0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 19, 32)               128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 9, 32)                0         ['conv1d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 7, 64)                6208      ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 3, 64)                0         ['conv1d_1[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 192)                  0         ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   12352     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   12352     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 64)                   12352     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " emotion (Dense)             (None, 6)                    390       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " age (Dense)                 (None, 8)                    520       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " gender (Dense)              (None, 2)                    130       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 44432 (173.56 KB)\n",
      "Trainable params: 44432 (173.56 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "149/149 [==============================] - 4s 10ms/step - loss: 3.8399 - emotion_loss: 1.5498 - age_loss: 1.6604 - gender_loss: 0.6298 - emotion_accuracy: 0.3688 - age_accuracy: 0.3507 - gender_accuracy: 0.6501 - val_loss: 3.6924 - val_emotion_loss: 1.5112 - val_age_loss: 1.5702 - val_gender_loss: 0.6110 - val_emotion_accuracy: 0.3661 - val_age_accuracy: 0.3955 - val_gender_accuracy: 0.6591\n",
      "Epoch 2/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.5760 - emotion_loss: 1.4463 - age_loss: 1.5468 - gender_loss: 0.5828 - emotion_accuracy: 0.4038 - age_accuracy: 0.3830 - gender_accuracy: 0.7031 - val_loss: 3.5801 - val_emotion_loss: 1.4826 - val_age_loss: 1.5139 - val_gender_loss: 0.5836 - val_emotion_accuracy: 0.3837 - val_age_accuracy: 0.4215 - val_gender_accuracy: 0.6935\n",
      "Epoch 3/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.4651 - emotion_loss: 1.4091 - age_loss: 1.4886 - gender_loss: 0.5674 - emotion_accuracy: 0.4324 - age_accuracy: 0.4074 - gender_accuracy: 0.7150 - val_loss: 3.5412 - val_emotion_loss: 1.4610 - val_age_loss: 1.4884 - val_gender_loss: 0.5918 - val_emotion_accuracy: 0.4055 - val_age_accuracy: 0.4291 - val_gender_accuracy: 0.6927\n",
      "Epoch 4/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.3784 - emotion_loss: 1.3915 - age_loss: 1.4380 - gender_loss: 0.5489 - emotion_accuracy: 0.4345 - age_accuracy: 0.4273 - gender_accuracy: 0.7257 - val_loss: 3.4779 - val_emotion_loss: 1.4531 - val_age_loss: 1.4391 - val_gender_loss: 0.5857 - val_emotion_accuracy: 0.3929 - val_age_accuracy: 0.4341 - val_gender_accuracy: 0.6927\n",
      "Epoch 5/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.3098 - emotion_loss: 1.3734 - age_loss: 1.3961 - gender_loss: 0.5403 - emotion_accuracy: 0.4385 - age_accuracy: 0.4549 - gender_accuracy: 0.7350 - val_loss: 3.4810 - val_emotion_loss: 1.4502 - val_age_loss: 1.4312 - val_gender_loss: 0.5995 - val_emotion_accuracy: 0.4097 - val_age_accuracy: 0.4425 - val_gender_accuracy: 0.6919\n",
      "Epoch 6/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.2444 - emotion_loss: 1.3546 - age_loss: 1.3623 - gender_loss: 0.5275 - emotion_accuracy: 0.4477 - age_accuracy: 0.4658 - gender_accuracy: 0.7396 - val_loss: 3.3989 - val_emotion_loss: 1.4186 - val_age_loss: 1.4112 - val_gender_loss: 0.5691 - val_emotion_accuracy: 0.4173 - val_age_accuracy: 0.4500 - val_gender_accuracy: 0.7145\n",
      "Epoch 7/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.1856 - emotion_loss: 1.3388 - age_loss: 1.3259 - gender_loss: 0.5209 - emotion_accuracy: 0.4620 - age_accuracy: 0.4861 - gender_accuracy: 0.7484 - val_loss: 3.4043 - val_emotion_loss: 1.4285 - val_age_loss: 1.3934 - val_gender_loss: 0.5824 - val_emotion_accuracy: 0.4139 - val_age_accuracy: 0.4618 - val_gender_accuracy: 0.6986\n",
      "Epoch 8/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.1352 - emotion_loss: 1.3213 - age_loss: 1.2998 - gender_loss: 0.5140 - emotion_accuracy: 0.4733 - age_accuracy: 0.4979 - gender_accuracy: 0.7484 - val_loss: 3.4527 - val_emotion_loss: 1.4386 - val_age_loss: 1.3857 - val_gender_loss: 0.6283 - val_emotion_accuracy: 0.4097 - val_age_accuracy: 0.4576 - val_gender_accuracy: 0.6835\n",
      "Epoch 9/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.0827 - emotion_loss: 1.3097 - age_loss: 1.2661 - gender_loss: 0.5069 - emotion_accuracy: 0.4738 - age_accuracy: 0.5181 - gender_accuracy: 0.7564 - val_loss: 3.4113 - val_emotion_loss: 1.4411 - val_age_loss: 1.3818 - val_gender_loss: 0.5885 - val_emotion_accuracy: 0.4274 - val_age_accuracy: 0.4677 - val_gender_accuracy: 0.7120\n",
      "Epoch 10/10\n",
      "149/149 [==============================] - 1s 6ms/step - loss: 3.0323 - emotion_loss: 1.2960 - age_loss: 1.2361 - gender_loss: 0.5001 - emotion_accuracy: 0.4882 - age_accuracy: 0.5239 - gender_accuracy: 0.7654 - val_loss: 3.3603 - val_emotion_loss: 1.4173 - val_age_loss: 1.3729 - val_gender_loss: 0.5701 - val_emotion_accuracy: 0.4291 - val_age_accuracy: 0.4643 - val_gender_accuracy: 0.7103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x272e12ec760>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('edited_file.csv')\n",
    "\n",
    "# Assuming the features are in columns 0 to 20, and the labels are in columns 21, 22, and 23\n",
    "X = df.iloc[:, :21].values\n",
    "y_emotion = df.iloc[:, 21].values  # Assuming 'emotion' is in column 21\n",
    "y_age = df.iloc[:, 22].values  # Assuming 'age' is in column 22\n",
    "y_gender = df.iloc[:, 23].values  # Assuming 'gender' is in column 23\n",
    "\n",
    "# Use LabelEncoder for 'emotion'\n",
    "label_encoder = LabelEncoder()\n",
    "y_emotion_encoded = label_encoder.fit_transform(y_emotion)\n",
    "num_emotion_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Convert categorical labels to one-hot encoding\n",
    "y_emotion = to_categorical(y_emotion_encoded, num_classes=num_emotion_classes)\n",
    "y_age = to_categorical(y_age, num_classes=8)\n",
    "y_gender = to_categorical(y_gender, num_classes=2)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_emotion_train, y_emotion_test, y_age_train, y_age_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    X, y_emotion, y_age, y_gender, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# CNN Layers\n",
    "inputs_cnn = Input(shape=(21, 1))\n",
    "conv1 = Conv1D(32, 3, activation='relu')(inputs_cnn)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(64, 3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "flat_cnn = Flatten()(pool2)\n",
    "\n",
    "# Dense Layers\n",
    "dense1 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense2 = Dense(64, activation='relu')(flat_cnn)\n",
    "dense3 = Dense(64, activation='relu')(flat_cnn)\n",
    "\n",
    "# Output Layers\n",
    "output_emotion = Dense(num_emotion_classes, activation='softmax', name='emotion')(dense1)\n",
    "output_age = Dense(8, activation='softmax', name='age')(dense2)\n",
    "output_gender = Dense(2, activation='softmax', name='gender')(dense3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs_cnn, outputs=[output_emotion, output_age, output_gender])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss={'emotion': 'categorical_crossentropy', 'age': 'categorical_crossentropy', 'gender': 'categorical_crossentropy'},\n",
    "              metrics={'emotion': 'accuracy', 'age': 'accuracy', 'gender': 'accuracy'})\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, [y_emotion_train, y_age_train, y_gender_train], epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40a881b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 4ms/step - loss: 3.3559 - emotion_loss: 1.4363 - age_loss: 1.3855 - gender_loss: 0.5341 - emotion_accuracy: 0.4171 - age_accuracy: 0.4641 - gender_accuracy: 0.7401\n",
      "Test Loss: 3.3559069633483887\n",
      "Emotion Accuracy: 53.41%\n",
      "Age Accuracy: 41.71%\n",
      "Gender Accuracy: 46.41%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluation = model.evaluate(X_test, [y_emotion_test, y_age_test, y_gender_test])\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Test Loss: {evaluation[0]}\")\n",
    "print(f\"Emotion Accuracy: {evaluation[3] * 100:.2f}%\")\n",
    "print(f\"Age Accuracy: {evaluation[4] * 100:.2f}%\")\n",
    "print(f\"Gender Accuracy: {evaluation[5] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e8f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0935978 ,  0.45867844,  0.14982726, ..., -0.6208005 ,\n",
       "        -0.42624283,  0.11530354],\n",
       "       [ 0.64861714, -0.27841971,  0.11638143, ..., -0.6857083 ,\n",
       "        -1.36419564, -1.46043764],\n",
       "       [-0.38355167, -0.10570402, -0.38531868, ..., -0.06991874,\n",
       "        -0.18096371, -0.34574716],\n",
       "       ...,\n",
       "       [-1.0120528 , -1.68747977, -1.17965692, ...,  0.27836867,\n",
       "        -0.8676738 ,  0.99067074],\n",
       "       [ 0.04059206, -0.37274808,  0.00504107, ...,  1.43886541,\n",
       "        -0.53332273,  0.56958108],\n",
       "       [ 0.32168892, -0.42729786, -0.27830956, ..., -0.23399819,\n",
       "        -1.39085161, -1.84397653]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3d095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import io\n",
    "\n",
    "def feature_extraction(audio, sampling_rate=16000):\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sampling_rate))\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sampling_rate))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate))\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate)\n",
    "    mfcc_features = [np.mean(el) for i, el in enumerate(mfcc) if i not in [11, 18]]\n",
    "\n",
    "    return np.array([spectral_centroid, spectral_bandwidth, spectral_rolloff] + mfcc_features)\n",
    "\n",
    "# Assuming you have a WAV file 'audio_file.wav' in the current directory\n",
    "file_path = 'test.wav'\n",
    "\n",
    "# Read the audio file\n",
    "with open(file_path, 'rb') as file:\n",
    "    audio, _ = librosa.load(io.BytesIO(file.read()), sr=16000)\n",
    "\n",
    "custom_input = feature_extraction(audio)\n",
    "\n",
    "# print(\"Extracted features:\")\n",
    "# print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd73128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "Emotion Prediction: [4]\n",
      "Age Prediction: [7]\n",
      "Gender Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Custom input\n",
    "# custom_input = np.array([[2679.9395691628692 ,3347.669488765762, 5745.486745886655\n",
    "#  -625.2181396484375, 111.32093811035156 ,6.3269944190979 ,34.75761413574219,\n",
    "#  31.619901657104492 ,-4.714645862579346 ,-0.486030638217926,\n",
    "#  -4.934024333953857 ,-12.714733123779297 ,-2.0551483631134033,\n",
    "#  -3.7411177158355713, -10.702962875366211, -11.20263671875,\n",
    "#  -12.003522872924805, -8.489580154418945 ,-5.463275909423828,\n",
    "#  -4.954216480255127, -3.715198278427124,-6.338474273681641]])\n",
    "\n",
    "# # Standardize the custom input\n",
    "# custom_input = scaler.transform(custom_input)\n",
    "\n",
    "# Reshape the input to match the model's input shape\n",
    "custom_input = custom_input.reshape((1, 21, 1))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(custom_input)\n",
    "\n",
    "# Extract the predictions for each output\n",
    "emotion_prediction = predictions[0]\n",
    "age_prediction = predictions[1]\n",
    "gender_prediction = predictions[2]\n",
    "\n",
    "# Decode emotion predictions using the LabelEncoder\n",
    "emotion_prediction_decoded = label_encoder.inverse_transform(np.argmax(emotion_prediction, axis=1))\n",
    "\n",
    "# Print the predictions\n",
    "print(f\"Emotion Prediction: {emotion_prediction_decoded}\")\n",
    "print(f\"Age Prediction: {np.argmax(age_prediction, axis=1)}\")\n",
    "print(f\"Gender Prediction: {np.argmax(gender_prediction, axis=1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b46a67df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\New folder\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model to a HDF5 file\n",
    "model.save('all_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd43182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
